---
layout:     post
title:      "Anyone Can Learn To Code an LSTM-RNN in Python (Part 2: LSTM)"
subtitle:   "Baby steps to your neural network's first memories."
date:       2015-11-19 12:00:00
author:     "iamtrask"
header-img: "img/nebula.jpg"
---

<p><b>Summary:</b> I learn best with toy code that I can play with. This tutorial teaches Long Short Term Memory, Recurrent Neural Networks via a very simple toy example, a short python implementation.</p>

<p>I'll tweet out further blogposts at <a href="https://twitter.com/iamtrask">@iamtrask</a>. Feel free to follow if you'd be interested in reading and thanks for all the feedback!
</p>

<h2 class="section-heading">Part 1: Oh Boy!</h2>

<p>This is by far the most daunting concept to make simple I've attempted so far. Before we get started, please be open to messaging/tweeting/emailing me with what "clicks" in your mind and what doesn't so that I can continue to refine the post. Even though they've been around for nearly a decade, LSTMs are very advanced and still not fully understood by the community. Also, even though I'll review some of the nuances, this blogpost assumes that you have read (or are already familiar with) the previous blogposts on backpropagation, gradient descent, and recurrent neural networks. Allright... let's do this!</p>

